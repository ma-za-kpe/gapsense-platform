#!/usr/bin/env python3
"""
Curriculum Loader: gapsense-data JSON â†’ PostgreSQL

Reads the prerequisite graph JSON from gapsense-data repo and loads it into
the PostgreSQL database tables.

Usage:
    python scripts/load_curriculum.py              # Load from default path
    python scripts/load_curriculum.py --reload     # Truncate and reload
    python scripts/load_curriculum.py --path=/custom/path.json
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path
from typing import Any
from uuid import UUID, uuid4

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from gapsense.config import settings
from gapsense.core.models import (
    CascadePath,
    CurriculumIndicator,
    CurriculumMisconception,
    CurriculumNode,
    CurriculumPrerequisite,
    CurriculumStrand,
    CurriculumSubStrand,
)
from gapsense.core.models.base import Base


class CurriculumLoader:
    """Loads prerequisite graph from JSON into PostgreSQL."""

    def __init__(self, db_url: str, graph_path: Path):
        self.db_url = db_url
        self.graph_path = graph_path
        self.engine = create_async_engine(db_url, echo=False)
        self.SessionLocal = sessionmaker(self.engine, class_=AsyncSession, expire_on_commit=False)

        # Track node code â†’ UUID mapping for prerequisites
        self.node_id_map: dict[str, UUID] = {}

    async def create_tables(self) -> None:
        """Create all tables if they don't exist."""
        async with self.engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        print("âœ… Database tables created/verified")

    async def truncate_tables(self) -> None:
        """Truncate all curriculum tables (for --reload)."""
        async with self.SessionLocal() as session:
            # Delete in reverse dependency order
            await session.execute(CascadePath.__table__.delete())
            await session.execute(CurriculumMisconception.__table__.delete())
            await session.execute(CurriculumIndicator.__table__.delete())
            await session.execute(CurriculumPrerequisite.__table__.delete())
            await session.execute(CurriculumNode.__table__.delete())
            await session.execute(CurriculumSubStrand.__table__.delete())
            await session.execute(CurriculumStrand.__table__.delete())
            await session.commit()
        print("âœ… Truncated all curriculum tables")

    async def load_graph(self) -> None:
        """Main loading orchestrator."""
        print(f"ğŸ“– Reading graph from: {self.graph_path}")

        if not self.graph_path.exists():
            raise FileNotFoundError(
                f"Prerequisite graph not found: {self.graph_path}\n"
                f"Make sure GAPSENSE_DATA_PATH points to gapsense-data repo."
            )

        with open(self.graph_path, encoding="utf-8") as f:
            data = json.load(f)

        print(f"ğŸ“Š Graph version: {data.get('metadata', {}).get('version', 'unknown')}")
        print(f"ğŸ“Š Nodes: {len(data.get('nodes', []))}")
        print(f"ğŸ“Š Prerequisites: {len(data.get('prerequisites', []))}")
        print(f"ğŸ“Š Misconceptions: {len(data.get('misconceptions', []))}")
        print(f"ğŸ“Š Cascades: {len(data.get('cascades', []))}")

        async with self.SessionLocal() as session:
            # Step 1: Load strands
            await self._load_strands(session, data.get("strands", []))

            # Step 2: Load sub-strands
            await self._load_sub_strands(
                session, data.get("sub_strands_by_phase", data.get("sub_strands", []))
            )

            # Step 3: Load nodes (returns extracted prerequisites from node objects)
            extracted_prerequisites = await self._load_nodes(session, data.get("nodes", []))

            # Step 4: Load prerequisites (edges)
            # Use embedded prerequisites if external list is empty
            prereq_list = data.get("prerequisites", [])
            if not prereq_list and extracted_prerequisites:
                prereq_list = extracted_prerequisites
            await self._load_prerequisites(session, prereq_list)

            # Step 5: Load misconceptions
            await self._load_misconceptions(session, data.get("misconceptions", []))

            # Step 6: Load cascade paths
            await self._load_cascades(
                session, data.get("cascades", data.get("critical_cascade_paths", []))
            )

            await session.commit()

        print("âœ… Curriculum loaded successfully!")

    async def _load_strands(
        self, session: AsyncSession, strands: list[dict[str, Any]] | dict[str, Any]
    ) -> None:
        """Load curriculum strands."""
        # Handle both list and dict formats
        if isinstance(strands, dict):
            strands_list = [
                {
                    "strand_number": int(k),
                    "name": v["name"],
                    "color_hex": v.get("color"),
                    "description": v.get("description"),
                }
                for k, v in strands.items()
            ]
        else:
            strands_list = strands

        for strand_data in strands_list:
            strand = CurriculumStrand(
                strand_number=strand_data["strand_number"],
                name=strand_data["name"],
                color_hex=strand_data.get("color_hex"),
                description=strand_data.get("description"),
            )
            session.add(strand)
        await session.flush()
        print(f"  âœ… Loaded {len(strands_list)} strands")

    async def _load_sub_strands(
        self, session: AsyncSession, sub_strands: list[dict[str, Any]] | dict[str, Any]
    ) -> None:
        """Load curriculum sub-strands."""
        # Handle both list and dict formats
        if isinstance(sub_strands, dict):
            # New format: sub_strands_by_phase
            sub_strands_list = []
            for phase, phase_sub_strands in sub_strands.items():
                if not isinstance(phase_sub_strands, dict):
                    continue
                for sub_strand_code, name in phase_sub_strands.items():
                    # Parse "1.1" -> strand_number = 1, sub_strand_number = 1
                    parts = sub_strand_code.split(".")
                    strand_number = int(parts[0])
                    sub_strand_number = int(parts[1])
                    sub_strands_list.append(
                        {
                            "strand_number": strand_number,
                            "sub_strand_number": sub_strand_number,
                            "phase": phase,
                            "name": name,
                        }
                    )
        else:
            sub_strands_list = sub_strands

        for sub_strand_data in sub_strands_list:
            # Look up strand_id from strand_number
            result = await session.execute(
                select(CurriculumStrand.id).where(
                    CurriculumStrand.strand_number == sub_strand_data["strand_number"]
                )
            )
            strand_id = result.scalar_one()

            sub_strand = CurriculumSubStrand(
                strand_id=strand_id,
                sub_strand_number=sub_strand_data["sub_strand_number"],
                phase=sub_strand_data["phase"],
                name=sub_strand_data["name"],
                description=sub_strand_data.get("description"),
            )
            session.add(sub_strand)
        await session.flush()
        print(f"  âœ… Loaded {len(sub_strands_list)} sub-strands")

    async def _load_nodes(
        self, session: AsyncSession, nodes: list[dict[str, Any]] | dict[str, Any]
    ) -> list[dict[str, Any]]:
        """Load curriculum nodes.

        Returns:
            List of prerequisite edges extracted from node objects
        """
        # Handle both list and dict formats
        if isinstance(nodes, dict):
            # Filter out non-dict values (section headers, etc.)
            nodes_list = [v for v in nodes.values() if isinstance(v, dict)]
        else:
            nodes_list = nodes

        extracted_prerequisites = []

        for node_data in nodes_list:
            # Parse code to extract strand and sub-strand numbers
            # Format: B2.1.1.1 = grade.strand.substrand.content
            code_parts = node_data["code"].split(".")
            grade = f"B{code_parts[0][1:]}"  # Extract grade (B2, B3, etc.)
            strand_num = int(code_parts[1])
            sub_strand_num = int(code_parts[2])
            content_standard_num = int(code_parts[3])

            # Look up strand_id
            result = await session.execute(
                select(CurriculumStrand.id).where(CurriculumStrand.strand_number == strand_num)
            )
            strand_id = result.scalar_one()

            # Look up sub_strand_id
            # Need to match on strand_id, sub_strand_number, and phase
            phase = self._grade_to_phase(grade)
            result = await session.execute(
                select(CurriculumSubStrand.id).where(
                    CurriculumSubStrand.strand_id == strand_id,
                    CurriculumSubStrand.sub_strand_number == sub_strand_num,
                    CurriculumSubStrand.phase == phase,
                )
            )
            sub_strand_id = result.scalar_one_or_none()
            if sub_strand_id is None:
                print(
                    f"âš ï¸  Skipping node {node_data['code']}: "
                    f"sub_strand not found (strand={strand_num}, sub={sub_strand_num}, phase={phase})"
                )
                continue

            # Generate UUID for this node
            node_id = uuid4()
            self.node_id_map[node_data["code"]] = node_id

            node = CurriculumNode(
                id=node_id,
                code=node_data["code"],
                grade=grade,
                strand_id=strand_id,
                sub_strand_id=sub_strand_id,
                content_standard_number=content_standard_num,
                title=node_data["title"],
                description=node_data["description"],
                severity=node_data["severity"],
                severity_rationale=node_data.get("severity_rationale"),
                questions_required=node_data.get("questions_required", 2),
                confidence_threshold=node_data.get("confidence_threshold", 0.80),
                ghana_evidence=node_data.get("ghana_evidence"),
                population_status=node_data.get("population_status", "skeleton"),
            )
            session.add(node)

            # Extract prerequisites from node (if embedded in node_data)
            if "prerequisites" in node_data and isinstance(node_data["prerequisites"], list):
                for prereq_code in node_data["prerequisites"]:
                    if prereq_code:  # Skip empty strings
                        extracted_prerequisites.append(
                            {
                                "source": prereq_code,
                                "target": node_data["code"],
                                "relationship": "requires",
                            }
                        )

        await session.flush()
        print(f"  âœ… Loaded {len(nodes_list)} nodes")

        if extracted_prerequisites:
            print(f"  ğŸ“‹ Extracted {len(extracted_prerequisites)} prerequisites from nodes")

        return extracted_prerequisites

    async def _load_prerequisites(
        self, session: AsyncSession, prerequisites: list[dict[str, Any]]
    ) -> None:
        """Load prerequisite edges."""
        for prereq_data in prerequisites:
            source_id = self.node_id_map[prereq_data["source"]]
            target_id = self.node_id_map[prereq_data["target"]]

            prereq = CurriculumPrerequisite(
                source_node_id=source_id,
                target_node_id=target_id,
                relationship_type=prereq_data.get("relationship", "requires"),
                weight=prereq_data.get("weight", 1.0),
                notes=prereq_data.get("notes"),
            )
            session.add(prereq)

        await session.flush()
        print(f"  âœ… Loaded {len(prerequisites)} prerequisite edges")

    async def _load_misconceptions(
        self, session: AsyncSession, misconceptions: list[dict[str, Any]]
    ) -> None:
        """Load common misconceptions."""
        for misconception_data in misconceptions:
            node_id = self.node_id_map[misconception_data["node_code"]]

            misconception = CurriculumMisconception(
                id=misconception_data["id"],
                node_id=node_id,
                description=misconception_data["description"],
                evidence=misconception_data["evidence"],
                root_cause=misconception_data["root_cause"],
                remediation_approach=misconception_data["remediation_approach"],
                frequency_estimate=misconception_data.get("frequency_estimate"),
                source_citation=misconception_data.get("source_citation"),
            )
            session.add(misconception)

        await session.flush()
        print(f"  âœ… Loaded {len(misconceptions)} misconceptions")

    async def _load_cascades(
        self, session: AsyncSession, cascades: list[dict[str, Any]] | dict[str, Any]
    ) -> None:
        """Load cascade failure paths."""
        # Handle different formats
        if isinstance(cascades, dict):
            # Check if it's metadata dict (v1.2 format) or actual cascades
            if "description" in cascades and "node_sequence" not in cascades:
                # This is just metadata, not actual cascade data
                print("  â„¹ï¸  No cascade path data in current format")
                return
            # Otherwise treat as single cascade
            cascades = [cascades]

        for cascade_data in cascades:
            # Skip if not a dict
            if not isinstance(cascade_data, dict):
                continue

            # Skip if doesn't have required fields
            if "node_sequence" not in cascade_data:
                continue

            # Convert node codes to UUIDs
            node_sequence = [self.node_id_map[code] for code in cascade_data["node_sequence"]]

            # Optional: entry point
            entry_point_id = None
            if cascade_data.get("diagnostic_entry_point"):
                entry_point_id = self.node_id_map[cascade_data["diagnostic_entry_point"]]

            cascade = CascadePath(
                name=cascade_data["name"],
                description=cascade_data["description"],
                frequency=cascade_data.get("frequency"),
                diagnostic_entry_point=entry_point_id,
                diagnostic_entry_question=cascade_data.get("diagnostic_entry_question"),
                remediation_priority=cascade_data.get("remediation_priority"),
                node_sequence=node_sequence,
            )
            session.add(cascade)

        await session.flush()
        if cascades and isinstance(cascades, list):
            print(f"  âœ… Loaded {len(cascades)} cascade paths")

    @staticmethod
    def _grade_to_phase(grade: str) -> str:
        """Convert grade (B1-B9) to phase (B1_B3, B4_B6, B7_B9)."""
        grade_num = int(grade[1:])
        if 1 <= grade_num <= 3:
            return "B1_B3"
        elif 4 <= grade_num <= 6:
            return "B4_B6"
        elif 7 <= grade_num <= 9:
            return "B7_B9"
        else:
            raise ValueError(f"Invalid grade: {grade}")

    async def verify_load(self) -> None:
        """Verify data was loaded correctly."""
        async with self.SessionLocal() as session:
            # Count records
            strand_count = (await session.execute(select(CurriculumStrand))).scalars().all()
            node_count = (await session.execute(select(CurriculumNode))).scalars().all()
            prereq_count = (await session.execute(select(CurriculumPrerequisite))).scalars().all()

            print("\nğŸ“Š Database Verification:")
            print(f"  Strands: {len(strand_count)}")
            print(f"  Nodes: {len(node_count)}")
            print(f"  Prerequisites: {len(prereq_count)}")

            # Sample query: Get a node with its prerequisites
            if node_count:
                sample_node = node_count[0]
                print(f"\nğŸ“ Sample Node: {sample_node.code} - {sample_node.title}")
                print(f"  Severity: {sample_node.severity}/5")
                print(f"  Grade: {sample_node.grade}")


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Load curriculum from JSON to PostgreSQL")
    parser.add_argument(
        "--reload",
        action="store_true",
        help="Truncate existing data before loading",
    )
    parser.add_argument(
        "--path",
        type=Path,
        help="Custom path to prerequisite graph JSON",
    )
    parser.add_argument(
        "--db-url",
        type=str,
        help="Custom database URL (default from settings)",
    )
    args = parser.parse_args()

    # Determine paths
    db_url = args.db_url or settings.DATABASE_URL
    graph_path = args.path or settings.prerequisite_graph_path

    print("ğŸš€ GapSense Curriculum Loader")
    print(f"ğŸ“ Graph: {graph_path}")
    print(f"ğŸ—„ï¸  Database: {db_url.split('@')[1] if '@' in db_url else db_url}\n")

    loader = CurriculumLoader(db_url, graph_path)

    try:
        # Create tables
        await loader.create_tables()

        # Truncate if --reload
        if args.reload:
            print("âš ï¸  RELOAD mode: Truncating existing data...")
            await loader.truncate_tables()

        # Load data
        await loader.load_graph()

        # Verify
        await loader.verify_load()

        print("\nâœ… Load complete!")

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        raise
    finally:
        await loader.engine.dispose()


if __name__ == "__main__":
    asyncio.run(main())
